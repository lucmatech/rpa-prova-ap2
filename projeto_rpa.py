import sqlite3
import requests
import urllib3

# Desativa avisos SSL temporariamente (caso necessÃ¡rio)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ===== Parte 1: API de paÃ­ses =====

def extrair_dados_paises():
    conn = sqlite3.connect("paises.db")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS paises (
            nome_comum TEXT,
            nome_oficial TEXT,
            capital TEXT,
            continente TEXT,
            regiao TEXT,
            sub_regiao TEXT,
            populacao INTEGER,
            area REAL,
            moeda_nome TEXT,
            moeda_simbolo TEXT,
            idioma TEXT,
            fuso_horario TEXT,
            url_bandeira TEXT
        )
    """)

    for i in range(3):
        pais = input(f"Digite o nome do paÃ­s #{i+1}: ").strip()

        try:
            resposta = requests.get(f"https://restcountries.com/v3.1/name/{pais}", verify=False)
            dados = resposta.json()[0]

            nome_comum = dados["name"]["common"]
            nome_oficial = dados["name"]["official"]
            capital = dados.get("capital", ["N/A"])[0]
            continente = dados.get("continents", ["N/A"])[0]
            regiao = dados.get("region", "N/A")
            sub_regiao = dados.get("subregion", "N/A")
            populacao = dados.get("population", 0)
            area = dados.get("area", 0)
            moedas = list(dados.get("currencies", {}).values())[0]
            moeda_nome = moedas.get("name", "N/A")
            moeda_simbolo = moedas.get("symbol", "N/A")
            idioma = list(dados.get("languages", {}).values())[0]
            fuso_horario = dados.get("timezones", ["N/A"])[0]
            url_bandeira = dados.get("flags", {}).get("png", "N/A")

            cursor.execute("""
                INSERT INTO paises VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (nome_comum, nome_oficial, capital, continente, regiao, sub_regiao,
                  populacao, area, moeda_nome, moeda_simbolo, idioma, fuso_horario, url_bandeira))

            print(f"âœ… Dados de {nome_comum} salvos com sucesso.\n")

        except Exception as e:
            print(f"Erro ao buscar dados de '{pais}': {e}")

    conn.commit()
    conn.close()

    
# ===== Parte 2: Web Scraping dos livros =====

from bs4 import BeautifulSoup
import sqlite3
import requests

def extrair_dados_livros():
    conn = sqlite3.connect("livraria.db")
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS livros (
            titulo TEXT,
            preco TEXT,
            avaliacao TEXT,
            disponibilidade TEXT
        )
    """)

    url_base = "https://books.toscrape.com/"
    resposta = requests.get(url_base)
    sopa = BeautifulSoup(resposta.content, "html.parser")
    livros = sopa.find_all("article", class_="product_pod")[:10]

    for livro in livros:
        titulo = livro.h3.a["title"]
        preco = livro.find("p", class_="price_color").text.strip()
        avaliacao = livro.p["class"][1]
        disponibilidade = livro.find("p", class_="instock availability").text.strip()

        cursor.execute("""
            INSERT INTO livros VALUES (?, ?, ?, ?)
        """, (titulo, preco, avaliacao, disponibilidade))

        print(f"ðŸ“š Livro salvo: {titulo}")

    conn.commit()
    conn.close()

# ===== ExecuÃ§Ã£o principal =====

if __name__ == "__main__":
    print("ðŸ”Ž Iniciando extraÃ§Ã£o de dados de paÃ­ses...")
    extrair_dados_paises()

    print("\nðŸ”Ž Iniciando extraÃ§Ã£o de dados de livros...")
    extrair_dados_livros()

    print("\nâœ… Etapas 1 e 2 concluÃ­das. Arquivos 'paises.db' e 'livraria.db' gerados.")
